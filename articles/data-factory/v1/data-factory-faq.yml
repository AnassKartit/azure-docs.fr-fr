### YamlMime:FAQ
metadata:
  title: Azure Data Factory : Forum aux Questions
  description: Forum aux Questions sur Azure Data Factory.
  author: dcstwh
  ms.author: weetok
  ms.reviewer: jburchel
  ms.service: data-factory
  ms.subservice: v1
  ms.topic: conceptual
  ms.date: 01/10/2018
  ms.openlocfilehash: 50def0d8edc5b522634073c0abd433c5d15f9e58
  ms.sourcegitcommit: f6e2ea5571e35b9ed3a79a22485eba4d20ae36cc
  ms.translationtype: HT
  ms.contentlocale: fr-FR
  ms.lasthandoff: 09/24/2021
  ms.locfileid: "128633314"
title: Azure Data Factory : Forum aux Questions
summary: >
  > [!NOTE]

  > Cet article s’applique à la version 1 de Data Factory. Si vous utilisez la version actuelle du service Data Factory, consultez [Forum aux questions - Data Factory](../frequently-asked-questions.yml).


  [!INCLUDE [updated-for-az](../../../includes/updated-for-az.md)]
sections:
- name: Questions générales
  questions:
  - question: >
      qu'est-ce qu'Azure Data Factory ?
    answer: "Data Factory est un service d’intégration de données dans le cloud qui **automatise le déplacement et la transformation des données**. À la manière d’une usine, qui utilise des machines visant à transformer des matières premières en produits manufacturés, Data Factory orchestre des services existants qui collectent des données brutes et les transforment en informations prêtes à l’emploi.\n\nData Factory vous permet de créer des flux de travail pilotés par les données pour déplacer des données entre des magasins de données locaux et cloud et traiter/transformer des données avec des services de calcul comme Azure HDInsight et Azure Data Lake Analytics. Après avoir créé un pipeline qui exécute l’action dont vous avez besoin, vous pouvez planifier son exécution périodique (toutes les heures, tous les jours, toutes les semaines, etc.).   \n\nPour plus d’informations, consultez [Vue d’ensemble et concepts clés](data-factory-introduction.md).\n"
  - question: >
      où puis-je trouver des informations de tarification pour Azure Data Factory ?
    answer: "Pour plus d’informations sur la tarification d’Azure Data Factory, consultez la [page de tarification de Data Factory](https://go.microsoft.com/fwlink/?LinkId=517777).  \n"
  - question: >
      comment prendre en main Azure Data Factory ?
    answer: >
      * Pour obtenir une vue d'ensemble d'Azure Data Factory, consultez [Présentation d'Azure Data Factory](data-factory-introduction.md).

      * Pour obtenir un tutoriel expliquant comment **copier/déplacer des données** au moyen de l’activité de copie, consultez [Copie de données depuis Stockage Blob Azure vers Azure SQL Database](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md).

      * Pour obtenir un didacticiel sur la façon de **transformer des données** au moyen de l’activité Hive de HDInsight. Consultez [Traiter des données en exécutant un script Hive sur un cluster Hadoop](data-factory-build-your-first-pipeline.md)
  - question: >
      Dans quelle région sera disponible Data Factory ?
    answer: >
      Data Factory est disponible dans les régions **USA Ouest** et **Europe Nord**. Les services de calcul et de stockage utilisés par les fabriques de données peuvent être proposés dans d'autres régions. Consultez [Régions prises en charge](data-factory-introduction.md#supported-regions).
  - question: >
      quelles sont les limites du nombre de fabriques de données/pipelines/activités/jeux de données ?
    answer: >
      Consultez la section **Limites d’Azure Data Factory** de l’article [Abonnement Azure et limites, quotas et contraintes du service](../../azure-resource-manager/management/azure-subscription-service-limits.md#data-factory-limits) .
  - question: >
      Qu’est-il possible de faire avec le service Azure Data Factory en termes de création/développement ?
    answer: >
      Vous pouvez créer des fabriques de données en utilisant un des outils/SDK suivants :


      * **Visual Studio** : vous pouvez utiliser Visual Studio pour créer une fabrique de données Azure. Pour plus d’informations, consultez [Créer votre premier pipeline de données à l’aide de Visual Studio](data-factory-build-your-first-pipeline-using-vs.md) .

      * **Azure PowerShell** : consultez [Créer et surveiller Azure Data Factory à l’aide d’Azure PowerShell](data-factory-build-your-first-pipeline-using-powershell.md) pour obtenir un didacticiel/une procédure pas à pas pour créer une fabrique de données à l’aide de PowerShell. Consultez les [Informations de référence sur les applets de commande Data Factory](/powershell/module/az.datafactory/) dans la bibliothèque MSDN pour obtenir une documentation complète sur les applets de commande Data Factory.

      * **Bibliothèque de classes .NET** Vous pouvez créer par programmation des fabriques de données à l'aide du Kit de développement logiciel (SDK) Data Factory .NET. Consultez [Créer, surveiller et gérer des fabriques de données à l'aide du Kit de développement logiciel (SDK) .NET](data-factory-create-data-factories-programmatically.md) pour découvrir comment créer une fabrique de données à l'aide du Kit de développement logiciel (SDK) .NET. Consultez les [Informations de référence sur la bibliothèque de classes Data Factory](/dotnet/api/microsoft.azure.management.datafactories.models) pour obtenir une documentation complète sur le Kit de développement logiciel (SDK) Data Factory .NET.

      * **API REST** Vous pouvez également utiliser l'API REST exposée par le service Azure Data Factory pour créer et déployer des fabriques de données. Consultez les [Informations de référence sur l’API REST Data Factory](/rest/api/datafactory/) pour obtenir une documentation complète de l’API REST Data Factory.

      * **Modèle Azure Resource Manager** Voir [Tutoriel : Concevoir votre première fabrique de données Azure à l’aide du modèle Azure Resource Manager](data-factory-build-your-first-pipeline-using-arm.md) pour plus de détails.
  - question: >
      Puis-je renommer une fabrique de données ?
    answer: >
      Non. Tout comme les autres ressources Azure, le nom d'une fabrique de données Azure ne peut pas être modifié.
  - question: >
      Puis-je déplacer une fabrique de données d’un abonnement Azure à un autre ?
    answer: >
      Oui. Utilisez le bouton **Déplacer** sur le panneau de votre fabrique de données comme indiqué dans le graphique ci-dessous :


      :::image type="content" source="media/data-factory-faq/move-data-factory.png" alt-text="Déplacer la fabrique de données":::
  - question: >
      Quels sont les environnements de calcul pris en charge par Data Factory ?
    answer: >
      Le tableau suivant fournit une liste d’environnements de calcul pris en charge par Data Factory et les activités qui peuvent s’exécuter sur ces derniers.


      | Environnement de calcul | activities |

      | --- | --- |

      | [Cluster HDInsight à la demande](data-factory-compute-linked-services.md#azure-hdinsight-on-demand-linked-service) ou [votre propre cluster HDInsight](data-factory-compute-linked-services.md#azure-hdinsight-linked-service) |[DotNet](data-factory-use-custom-activities.md), [Hive](data-factory-hive-activity.md), [Pig](data-factory-pig-activity.md), [MapReduce](data-factory-map-reduce.md), [Diffusion en continu Hadoop](data-factory-hadoop-streaming-activity.md) |

      | [Azure Batch](data-factory-compute-linked-services.md#azure-batch-linked-service) |[DotNet](data-factory-use-custom-activities.md) |

      | [ML Studio (classique)](data-factory-compute-linked-services.md#ml-studio-classic-linked-service) |[Activités Studio (classique) : exécution par lot et ressource de mise à jour](data-factory-azure-ml-batch-execution-activity.md) |

      | [Service Analytique Azure Data Lake](data-factory-compute-linked-services.md#azure-data-lake-analytics-linked-service) |[Langage U-SQL du service Analytique Data Lake](data-factory-usql-activity.md) |

      | [Azure SQL](data-factory-compute-linked-services.md#azure-sql-linked-service), [Azure Synapse Analytics](data-factory-compute-linked-services.md#azure-synapse-analytics-linked-service), [SQL Server](data-factory-compute-linked-services.md#sql-server-linked-service) |[Procédure stockée](data-factory-stored-proc-activity.md) |
  - question: "Comparaison d’Azure Data Factory avec SQL Server Integration Services (SSIS) \n"
    answer: "Consultez la présentation [Azure Data Factory et SSIS](https://www.sqlbits.com/Sessions/Event15/Azure_Data_Factory_vs_SSIS) faite par l’un de nos MVP (Most Valued Professional), Reza Rad. Certains des changements récents Data Factory peuvent ne pas figurer dans le jeu de diapositives. Nous ajoutons continuellement des fonctionnalités à Azure Data Factory. Nous ajoutons continuellement des fonctionnalités à Azure Data Factory. Nous intégrerons ces mises à jour dans la comparaison des technologies d’intégration de données de Microsoft un peu plus tard cette année.   \n"
- name: Activités - Forum Aux Questions
  questions:
  - question: >
      Quels sont les différents types d’activités que vous pouvez utiliser dans un pipeline Data Factory ?
    answer: >
      * [Activités de déplacement des données](data-factory-data-movement-activities.md) pour déplacer les données.

      * [Activités de transformation des données](data-factory-data-transformation-activities.md) pour traiter/transformer les données.
  - question: >
      Quand une activité s'exécute-t-elle ?
    answer: >
      Les paramètres de configuration de la **disponibilité** présents dans la table de données de sortie déterminent quand l'activité doit être exécutée. Si des jeux de données d’entrée sont spécifiés, l’activité vérifie si toutes les dépendances de données d’entrée sont satisfaites (par exemple, l’état **Prêt** ) avant de s’exécuter.
- name: Activité de copie - Forum Aux Questions
  questions:
  - question: >
      Est-il préférable d'avoir un pipeline avec plusieurs activités ou un pipeline distinct pour chaque activité ?
    answer: >
      Les pipelines sont censés regrouper des activités connexes. Vous pouvez conserver les activités dans un seul pipeline si les tables qui les relient ne sont pas utilisées par d’autres activités extérieures au pipeline. De cette façon, vous n'aurez pas besoin de relier les périodes actives du pipeline pour qu'elles s'accordent les unes avec les autres. En outre, l’intégrité des données dans les tables internes au pipeline est mieux préservée lors de la mise à jour du pipeline. La mise à jour d'un pipeline arrête toutes les activités du pipeline, les supprime et les crée de nouveau. En termes de création, il peut être plus facile de voir le flux de données au sein des activités connexes dans un seul fichier JSON pour le pipeline.
  - question: >
      Quelles sont les banques de données prises en charge ?
    answer: >
      L’activité de copie dans Data Factory permet de copier les données d’un magasin de données source vers un magasin de données récepteur. Data Factory prend en charge les magasins de données suivants. Les données de n’importe quelle source peuvent être écrites dans n’importe quel récepteur. Cliquez sur une banque de données pour découvrir comment copier des données depuis/vers cette banque.


      [!INCLUDE [data-factory-supported-data-stores](includes/data-factory-supported-data-stores.md)]


      > [!NOTE]

      > Les banques de données signalées par un astérisque (*) peuvent être locales ou résider sur une instance Azure IaaS. Elles nécessitent que vous installiez une [passerelle de gestion des données](data-factory-data-management-gateway.md) sur un ordinateur local ou Azure IaaS.
  - question: >
      Quels sont les formats de fichier pris en charge ?
    answer: >
      Azure Data Factory prend en charge les types de format de fichier suivants :


      * [Format Texte](data-factory-supported-file-and-compression-formats.md#text-format)

      * [Format JSON](data-factory-supported-file-and-compression-formats.md#json-format)

      * [Format Avro](data-factory-supported-file-and-compression-formats.md#avro-format)

      * [Format ORC](data-factory-supported-file-and-compression-formats.md#orc-format)

      * [Format Parquet](data-factory-supported-file-and-compression-formats.md#parquet-format)
  - question: >
      Où est effectuée l’opération de copie ?
    answer: >
      Consultez la section relative au [déplacement des données disponibles à l’échelle mondiale](data-factory-data-movement-activities.md#global) pour plus d’informations. En bref, lorsqu’il s’agit d’un magasin de données local, l’opération de copie est effectuée par la passerelle de gestion des données dans votre environnement local. Lorsque le déplacement des données se fait entre deux magasins cloud, l’opération de copie est effectuée dans la région la plus proche de l’emplacement du récepteur dans la même zone géographique.
- name: Activité de HDInsight - Forum Aux Questions
  questions:
  - question: >
      Quelles sont les régions prises en charge par HDInsight ?
    answer: >
      Consultez la page [Tarification relative à HDInsight](https://azure.microsoft.com/pricing/details/hdinsight/) ou la section Disponibilité géographique dans l’article suivant.
  - question: >
      quelle est la région utilisée par un cluster HDInsight à la demande ?
    answer: "Le cluster HDInsight à la demande est créé dans la même région que le stockage que vous avez spécifié pour être utilisé avec le cluster.    \n"
  - question: >
      Comment associer des comptes de stockage supplémentaires à votre cluster HDInsight ?
    answer: >
      Si vous utilisez votre propre cluster HDInsight (BYOC, Bring Your Own Cluster), consultez les rubriques suivantes :


      * [Utilisation d’un cluster HDInsight avec des comptes de stockage et des metastores secondaires](https://social.technet.microsoft.com/wiki/contents/articles/23256.using-an-hdinsight-cluster-with-alternate-storage-accounts-and-metastores.aspx)

      * [Utilisation de comptes de stockage supplémentaires avec Hive HDInsight](/archive/blogs/cindygross/use-additional-storage-accounts-with-hdinsight-hive)


      Si vous utilisez un cluster à la demande créé par le service Data Factory, spécifiez les comptes de stockage supplémentaires pour le service lié HDInsight afin que le service Data Factory puisse les inscrire en votre nom. Dans la définition JSON pour le service lié à la demande, utilisez la propriété **additionalLinkedServiceNames** pour spécifier les comptes de stockage secondaires, comme indiqué dans l'extrait de code JSON suivant :


      ```JSON

      {
          "name": "MyHDInsightOnDemandLinkedService",
          "properties":
          {
              "type": "HDInsightOnDemandLinkedService",
              "typeProperties": {
                  "version": "3.5",
                  "clusterSize": 1,
                  "timeToLive": "00:05:00",
                  "osType": "Linux",
                  "linkedServiceName": "LinkedService-SampleData",
                  "additionalLinkedServiceNames": [ "otherLinkedServiceName1", "otherLinkedServiceName2" ]
              }
          }
      }

      ```

      Dans l'exemple ci-dessus, otherLinkedServiceName1 et otherLinkedServiceName2 représentent les services liés dont les définitions contiennent des informations d'identification nécessaires au cluster HDInsight pour accéder aux comptes de stockage secondaires.
- name: Tranches - Forum Aux Questions
  questions:
  - question: >
      Pourquoi mes tranches d’entrée ne sont-elles pas à l’état Prêt ?
    answer: "L’une des erreurs les plus courantes est de ne pas définir la propriété **external** sur **true** dans le jeu de données d’entrée quand les données d’entrée sont externes à la fabrique de données (c’est-à-dire qu’elles n’ont pas été générées par celle-ci).\n\nDans l’exemple suivant, vous devez uniquement définir **external** sur true pour **dataset1**.  \n\n**DataFactory1** Pipeline 1 : dataset1 -&gt; activity1 -&gt; dataset2 -&gt; activity2 -&gt; dataset3 Pipeline 2 : dataset3-&gt; activity3 -&gt; dataset4\n\nSi vous avez une autre fabrique de données avec un pipeline qui prend dataset4 (généré par le pipeline 2 dans DataFactory1), marquez dataset4 comme un jeu de données externe, car il a été généré par une autre fabrique de données (DataFactory1, et non DataFactory2).  \n\n**DataFactory2**    \nPipeline 1 : dataset4->activity4->dataset5\n\nSi la propriété « external » est définie correctement, vérifiez que les données d’entrée se trouvent bien dans l’emplacement spécifié dans la définition du jeu de données d’entrée.\n"
  - question: >
      Comment choisir une heure d’exécution autre que minuit pour une tranche qui est générée quotidiennement ?
    answer: "Utilisez la propriété **offset** pour spécifier l’heure à laquelle la tranche doit être générée. Pour plus d’informations sur cette propriété, consultez la section [Disponibilité du jeu de données](data-factory-create-datasets.md#dataset-availability) . Voici un exemple rapide :\n\n```json\n\"availability\":\n{\n    \"frequency\": \"Day\",\n    \"interval\": 1,\n    \"offset\": \"06:00:00\"\n}\n```\nLes tranches quotidiennes commencent à **6 h** au lieu de minuit, qui est l’heure par défaut.     \n"
  - question: >
      Comment puis-je réexécuter une tranche ?
    answer: "Vous pouvez réexécuter une tranche de l'une des manières suivantes :\n\n* Utilisez l’application Surveiller et gérer pour réexécuter une fenêtre d’activité ou une tranche. Pour obtenir des instructions, consultez [Réexécuter les fenêtres d’activité sélectionnées](data-factory-monitor-manage-app.md#perform-batch-actions) .   \n* Cliquez sur **Exécuter** dans la barre de commandes du panneau **TRANCHE DE DONNÉES** de la tranche, dans le portail Azure.\n* Exécutez l’applet de commande **Set-AzRmDataFactorySliceStatus** en ayant affecté la valeur **En attente** à l’état de la tranche.   \n\n    ```powershell\n    Set-AzDataFactorySliceStatus -Status Waiting -ResourceGroupName $ResourceGroup -DataFactoryName $df -TableName $table -StartDateTime \"02/26/2015 19:00:00\" -EndDateTime \"02/26/2015 20:00:00\"\n    ```\n  Consultez [Set-AzDataFactorySliceStatus](/powershell/module/az.datafactory/set-Azdatafactoryslicestatus) pour plus d’informations sur l’applet de commande.\n"
  - question: >
      Combien de temps dure le traitement d'une tranche ?
    answer: "Utilisez l’Explorateur de fenêtres d’activité dans l’application Surveiller et gérer pour connaître la durée du traitement d’une tranche de données. Pour plus d’informations, consultez [Explorateur de fenêtres d’activité](data-factory-monitor-manage-app.md#activity-window-explorer) .\n\nVous pouvez également faire ce qui suit dans le portail Azure :  \n\n1. Cliquez sur la vignette **Jeux de données** dans le panneau **DATA FACTORY** de votre fabrique de données.\n2. Cliquez sur le jeu de données spécifique dans le panneau **Jeux de données** .\n3. Sélectionnez la tranche qui vous intéresse dans la liste **Tranches récentes** située dans le panneau **TABLE**.\n4. Cliquez sur l'exécution dans la liste **Exécutions d’activité** située dans le panneau **TRANCHE DE DONNÉES**.\n5. Cliquez sur la vignette **Propriétés** dans le panneau **DÉTAILS DE L’EXÉCUTION D’ACTIVITÉ**.\n6. Une valeur doit apparaître dans le champ **DURÉE** . Il s’agit du temps nécessaire pour traiter la tranche.   \n"
  - question: >
      Comment arrêter une tranche en cours d'exécution ?
    answer: >-
      Si vous devez arrêter l’exécution du pipeline, vous pouvez utiliser l’applet de commande [Suspend-AzDataFactoryPipeline](/powershell/module/az.datafactory/suspend-azdatafactorypipeline) . Actuellement, l'interruption du pipeline n'arrête pas les exécutions de tranche en cours. Une fois que les exécutions en cours sont terminées, aucune tranche supplémentaire n'est récupérée.


      Si vous voulez vraiment arrêter immédiatement toutes les exécutions, le seul moyen est de supprimer le pipeline et de le recréer. Si vous choisissez de supprimer le pipeline, il est INUTILE de supprimer les tables et les services liés qu'il utilise.
